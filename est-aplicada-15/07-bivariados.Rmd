---
title: "Datos bivariados"
author: "Felipe González"
output:
  html_document:
    css: ../estilos/cajas.css
    theme: spacelab
---

```{r setup, include = FALSE}
library(dplyr)
library(tidyr)
knitr::opts_chunk$set(comment=NA, fig.align="center")
#options(digits = 4)
source("../codigo/tema_ggplot.R")
```


En la sección anterior estudiamos las relaciones entre dos variables: una 
categórica y una cuantitativa (tesitura vs. estatura, estado de la república
vs. ventas, etc.). Ahora veremos como entender la estructura de dependencia de
dos variables cuantitativas o de dos variables categóricas, también 
describiremos técnicas para estudiar más de dos variables a la vez. Comenzamos
con el caso de dos variables cuantitativas.

## Gráficas de dispersión en dos dimensiones y suavizamiento
Las gráficas de dispersión son la herramienta básica para describir la 
relación entre dos variables cuantitativas.

#### Ejemplo: ventas de una lotería.
Los siguientes datos muestran los premios ofrecidos y las ventas totales de una
lotería a lo largo de 53 sorteos (las unidades son cantidades de dinero 
indexadas). Una gráfica de dispersión muestra una relación clara entre premio
ofrecido y ventas:

```{r, fig.width=3.6, fig.height=3.3}
library(ggplot2)

# cargamos los datos
load("datos/ventas_sorteo.Rdata")
# desplegamos las primeras lineas
head(ventas.sorteo)

# gráfica de dispersión
ggplot(ventas.sorteo, aes(x = premio, y = ventas.tot.1)) + 
  geom_point()
```

En este ejemplo, las mediciones de ventas y tamaño del premio principal vienen
en pares: en la tabla de datos cada renglón corresponde a un sorteo, y tenemos 
dos columnas de datos, una de ventas y otra del premio, estos son datos 
bivariados. En este caso es claro que podemos llamar a la primera *respuesta* y 
a la segunda *factor*, aunque nuestros métodos no dependen de una interpretación
de este tipo. 

En la gráfica anterior observamos que la variación de unos pocos puntos 
(correspondientes a mayores premios) ocupa la mayor parte de la gráfica. Podemos
ver con más detalle el rango completo de los datos si repetimos esta gráfica
en escalas logarítmicas.

```{r, fig.width=6.6, fig.height=3.3}
library(gridExtra)

# usamos logaritmo base 10 y etiquetamos en la escala original
disp_log10 <- ggplot(ventas.sorteo, aes(x = (premio), y = (ventas.tot.1))) +
  geom_point() + 
  scale_x_log10(breaks = c(20000, 40000, 80000,10000)) +
  scale_y_log10(breaks = c(8000, 12000, 18000, 27000))

# usamos logaritmo natural
disp_log <- ggplot(ventas.sorteo, aes(x = log(premio), y = log(ventas.tot.1))) +
  geom_point()

grid.arrange(disp_log10, disp_log, ncol = 2)
```

Notemos que la relación entre estas dos variables no es lineal. Por ejemplo, 
para un premio de alrededor de 22,000 ($e^{10}$), un incremento de alrededor del 
25% en el premio resulta en incrementos de alrededor de 10% en las ventas. Pero
un premio de alrededor de 36,000 ($e^{10.5}$), el mismo incremento en el premio
resulta en un alza de 20% en las ventas.

Podemos apreciar mejor la relación si agregamos una curva *loess* que suavice.

```{r, fig.width=3.6, fig.height=3.3}
ggplot(ventas.sorteo, aes(x = log(premio), y = log(ventas.tot.1))) + 
  geom_point() +
  geom_smooth(method = "loess", alpha = 0.5, degree = 1, se = FALSE)
```

El patrón no era difícil de ver en los datos originales, sin embargo, la curva 
lo hace más claro, el logaritmo de las ventas tiene una relación no lineal con 
el logaritmo del premio: para premios no muy grandes no parece haber gran 
diferencia, pero cuando los premios empiezan a crecer por encima de 20,000
(aproximadamente $e^{10}$), las ventas crecen más rápidamente que para premios
menores. Este efecto se conoce como *bola de nieve*, y es frecuente en este 
tipo de loterías.

### Suavizamiento loess

Seguimos a Cleveland en la explicación del suavizamiento loess. Hay dos 
versiones: robusta y no robusta. Comenzamos explicando cómo se ajustan 
familias paramétricas de curvas a conjuntos de datos dados.

**Ajustando familias paramétricas.** Supongamos que tenemos la familia 
$f_{a,b}=ax+b$ y datos bivariados $(x_1,y_1), ..., (x_n, y_n)$. Buscamos 
encontrar $a$ y $b$ tales que $f_{a,b}$ de un ajuste *óptimo* a los datos. El
criterio de mínimos cuadrados consiste en encontrar $a$, $b$ que minimicen la
suma de cuadrados:

$$\sum_{i=1}^n(y_i-ax_i-b)^2$$

En este caso, las constantes $a$ y $b$ se pueden encontrar diferenciando esta
función objetivo. En este caso, estamos ajustando una recta a los datos, pero
podemos repetir el argumento con otras familias de funciones (por ejemplo 
cuadráticas). Para nuestro ejemplo obtenemos:

```{r, fig.width=3.6, fig.height=3.3}
ggplot(ventas.sorteo, aes(x = log(premio), y = log(ventas.tot.1))) + 
  geom_point() +
  geom_smooth(method = "lm")
```

Donde los parámetros $a$ y $b$ están dados por:

```{r}
mod_lineal <- lm(log(ventas.tot.1) ~ log(premio), data = ventas.sorteo)
round(coef(mod_lineal), 2)
```

De modo que la curva ajustada es $\log(V) = 4.6 + 0.47 \log(P)$, o en las 
unidades originales $V = 100 P^{0.47}$, donde $V$ son las ventas y $P$ el 
premio. Si observamos la gráfica notamos que este modelo lineal (en los 
logaritmos) no es adecuado para estos datos. Podríamos experimentar con otras
familias (por ejemplo, una cuadrática o cúbica, potencias, exponenciales, etc.);
sin embargo, en la etapa exploratoria es mejor tomar una ruta de ajuste más
flexibles (aún cuando esta no sea con funciones algebráicas), que al mismo 
tiempo sea robusto.

**Observación:** Los modelos lineales, cuando se pueden ajustar de manera 
razonable, son altamente deseables por su simplicidad: los datos se describen 
con pocos parámetros y tenemos incrementos marginales constantes en todo el 
rango de la variable que juega como factor, de modo que la interpretación es 
simple. Por esta razón, muchas veces vale la pena transformar los datos con 
el fin de *enderezar* la relación de dos variables y poder ajustar una función
lineal (lo veremos más adelante).

### Ajustando curvas loess
La idea es producir ajustes locales de rectas o funciones cuadráticas. En estas
familias es necesario especificar dos parámetros:

* Parámetro de suavizamiento $\alpha$: cuando $\alpha$ es más grande, la curva
ajustada es más suave.

* Grado de los polinomios locales que ajustamos $\lambda$: generalmente se 
toma $\lambda=1,2$.

Entonces, supongamos que los datos están dados por $(x_1,y_1), ..., (x_n, y_n)$,
y sean $\alpha$ un parámetro de suavizamiento fijo, y $\lambda=1$. Denotamos
como $\hat{g}(x)$ la curva loess ajustada, y como $w_i(x)$ a una función de peso
(que depende de x) para la observación $(x_i, y_i)$.

Para poder calcular $w_i(x)$ debemos comenzar calculando 
$q=\lfloor{n\alpha}\rfloor$ que suponemos mayor que uno. Ahora definimos la 
función *tricubo*:

$$
\begin{equation}
  T(u)=\begin{cases}
    (1-|u|^3)^3, & \text{para $|u| < 1$}.\\
    0, & \text{en otro caso}.
  \end{cases}
\end{equation}
$$

entonces, para el punto $x$ definimos el peso correspondiente al dato $(x_i,y_i)$, 
denotado por $w_i(x)$ como:

$$w_i(x)=T\bigg(\frac{|x-x_i|}{d_q(x)}\bigg)$$

donde $d_q(x)$ es el valor de la $q-ésima$ distancia más chica (la más grande 
entre las $q$ más chicas) entre los valores $|x-x_j|$, $j=1,2,...,n$. De esta 
forma, las observaciones $x_i$ reciben más peso cuanto más cerca estén de $x$. 

En palabras, de $x_1,...,x_n$ tomamos los $q$ datos más cercanos a $x$, que 
denotamos $x_{i_1}(x) \leq x_{i_2}(x) \leq \cdots x_{i_q}(x) \leq$. Los 
re-escalamos a $[0,1]$ haciendo corresponder $x$ a $0$ y el punto más alejado de
$x$ (que es $x_{i_q}$) a 1.

Aplicamos el tricubo (gráfica de abajo), para encontrar los pesos de cada punto.
Los puntos que están a una distancia mayor a $d_q(x)$ reciben un peso de cero, y 
los más cercanos un peso que depende de que tan cercanos están a $x$.

```{r, fig.width=3.4, fig.height=3.5}
tricubo <- function(x) {
  ifelse(abs(x) < 1, (1 - abs(x) ^ 3) ^ 3, 0)
}
curve(tricubo, from = -1.5, to = 1.5)
```

Finalmente ajustamos una recta de mínimos cuadrados ponderados por los pesos
$w_i(x)$, es decir, minimizamos

$$\sum_{i=1}^nw_i(x)(y_i-ax_i-b)^2$$

Hacemos esto para cada valor de $x$ que está en el rango de los datos 
$x_1,...,x_n$.

**Observaciones:** 

1. Cualquier función con la forma de flan del tricubo (se 
desvanece fuera de $(-1,1)$, es creciente en $(-1,0)$ y decreciente en $(0, 1)$,
además de ser continua y quizás diferenciable) es un buen candidato para usar 
en lugar del tricubo. La razón por la que escogemos precisamente esta forma 
algebráica no tiene que ver con el análisis exploratorio, sino con las ventajas
teóricas adicionales que tiene en la inferencia.

2. El caso $\lambda=2$ es similar. La única diferencia es en el paso de ajuste, 
donde usamos funciones cuadráticas, y obtendríamos entonces tres parámetros 
$a(x), b(x), c(x)$.

**Escogiendo de los parámetros.** Los parámetros $\alpha$ y $\lambda$ se 
encuentran por ensayo y error. La idea general es que debemos encontrar una 
curva que explique patrones importantes en los datos (que *ajuste* los datos)
pero que no muestre variaciones a escalas más chicas difíciles de explicar (que 
pueden ser el resultado de influencias de otras variables, variación muestral, 
ruido o errores de redondeo, por ejemplo). En el proceso de prueba y error 
iteramos el ajuste y en cada paso hacemos análisis de residuales, con el fin 
de seleccionar un suavizamiento adecuado.

### Suavizamiento y residuales

La curva loess se puede ver según el patrón:

<center> Valor Observado = Valor Ajustado + Residual </center>

Si $\hat{g}(x)$ es la curva de loess ajustada a los datos 
$(x_1,y_1), ..., (x_n, y_n)$, entonces tenemos que 

$$y_i=\hat{g}(x_i)+ \epsilon_i$$

con lo que adjudicaremos cierta variación a nuestro modelo (la curva loess) y 
otra a residuales no explicados. Los mismos principios para el análisis de 
residuales que estudiamos en la sección anterior aplican aquí.

#### Gráfica de dependencia de residuales
La primera gráfica que podemos hacer para juzgar el ajuste de nuestra curva es
una de dependencia de residuales. Para esto comenzamos ajustando nuestra curva
y después seguimos la fórmula de arriba  para calcular $\epsilon_i$. Graficamos
los pares $(x_i, \epsilon_i)$. 

En nuestro ejemplo de las ventas de la lotería, usamos $\lambda=1$ y $\alpha = 1$
(tomamos todos los puntos para hacer la estimación, con los puntos ponderados
por el tricubo).

```{r, fig.width=6.6, fig.height=3.3}
ajuste.loess <- loess(log(ventas.tot.1) ~ log(premio), data = ventas.sorteo,
  span = 1, degree = 1, family = "gaussian")
ventas.sorteo$ajustado <- ajuste.loess$fitted
ventas.sorteo$residual <- ajuste.loess$residuals

ajuste <- ggplot(ventas.sorteo, aes(x = log(premio), y = log(ventas.tot.1))) +
  geom_point() + 
  geom_line(data=ventas.sorteo, aes(x = log(premio), y = ajustado), col = 'red')

residual <- ggplot(ventas.sorteo, aes(x = log(premio), y = residual)) +
  geom_point() +
  geom_smooth(method = "loess", span = 1, degree = 1, se = FALSE)

grid.arrange(ajuste, residual, ncol = 2)
```

Observemos que también suavizamos los residuales para detectar patrones, en 
este caso, con  $\lambda=1$ y $\alpha = 1$, el análisis de residuales muestra un 
desajuste importante en la curva loess de los datos originales.

<div class="caja">
**Gráfica de dependencia de residuales.** Si en la gráfica de dependencia de 
residuales observamos patrones fuertes, quiere decir que el residual depende de
los valores del factor (en el eje horizontal). Lo malo de esto es que no hemos
simplificado mucho la descripción de la dependencia de la respuesta en el 
factor: no solo el ajustado varía con el factor, sino que la distribución de los 
residuales también varía con el factor, y no podemos hacer *pooling* para 
describir de manera más simple los datos.
</div>

Intentemos suavizando menos:

```{r, fig.width=6.6, fig.height=3.3}
ajuste.loess <- loess(log(ventas.tot.1) ~ log(premio), data = ventas.sorteo,
  span = 0.5, degree = 1, family = "gaussian")
ventas.sorteo$ajustado <- ajuste.loess$fitted
ventas.sorteo$residual <- ajuste.loess$residuals

ajuste <- ggplot(ventas.sorteo, aes(x = log(premio), y = log(ventas.tot.1))) +
  geom_point() + 
  geom_line(data=ventas.sorteo, aes(x = log(premio), y = ajustado), col = 'red')

residual <- ggplot(ventas.sorteo, aes(x = log(premio), y = residual)) +
  geom_point() +
  geom_smooth(method = "loess", span = 1, degree = 1, se = FALSE) +
  geom_hline() +
  ylim(c(-0.4, 0.4))

grid.arrange(ajuste, residual, ncol = 2)
```

En esta gráfica es difícil discernir un patrón furte que hayamos dejado sin 
explicar, con la excepción de un ligero incremento para los premios más grandes.
Ya no es necesario hacer cambios en el suavizamiento. 

<div class="caja">
Si suavizamos demasiado, decimos que *sobreajustamos* curvas de loess: esto 
sucede cuando nuestra curva sigue demasiado cerca los datos. La desventaja es 
que en estos casos el modelo es cercano en complejidad a los datos originales, 
es decir, esto es cercano a simplemente escribir los datos de otra manera, y no 
estamos suavizando variación a escala más pequeña que es menos importante.
</div>

Por tanto, una manera de encontrar buenos parámetros de suavizamiento es:

<div class="caja">
Para escoger el suavizamiento $\alpha$:

* Empezamos con $\alpha$ grande (mucho suavizamiento).

* Hacemos gráficas de dependencia de residuales para el ajuste. Suavizamos los
residuales (con mucho suavizamiento).

* Si existe un patrón claro en los residuales, reducir $\alpha$ y repetir.

* Si el suavizamiento de los residuales da una recta casi constante, terminamos
y no es necesario suavizar menos.
</div>

### Gráficas de nivel contra dispersión

Si queremos obtener una descripción eficiente de los datos, tenemos que 
verificar, como antes, que la distribución de los residuales no depende del 
valor ajustado. Una parte del trabajo ya la hemos hecho asegurándonos de que no
existen variaciones grandes en la media de los residuales al variar el factor.

Ahora nos concentramos en que la dispersión de los residuales no dependa del 
valor del factor (por ejemplo, que no haya dispersión monótona). Si la 
dispersión de los residuales varía con los valores ajustados, entonces la 
descripción de los datos mediante una curva loess no es suficiente, sino que
también necesitamos indicadores de la dispersión para cada nivel de los valores
ajustados. Si es posible, debemos evitar esta complejidad adicional: si los 
residuales tienen dispersión homogénea, entonces podemos usar nuestras técnicas
de residuales agrupados para describir con una sola distribución.

Una manera de hacer esto es graficando valores ajustados contra residuales en 
valor absoluto. Usualmente se prefiere graficar la raíz de los valores absolutos 
de los residuales, lo que implica protegernos, en cierta medida, de residuales
muy grandes.

En nuestro ejemplo, obtenemos la siguiente gráfica,

```{r, fig.width=3.6, fig.height=3.3}
ventas.sorteo$residual.raiz <- sqrt(abs(ventas.sorteo$residual))
ggplot(ventas.sorteo, aes(x=ajustado, y=residual.raiz)) +
  geom_point() + 
  geom_smooth(method = 'loess', span = 1, degree=1, se = FALSE) +
  geom_hline()
```

donde vemos que hay una relación entre los valores ajustados y la dispersión de
los residuales: hay un ligero efecto de dispersión monótona. Es decir, vemos
evidencia de dispersión no homogénea de los residuales (aunque esta no es muy
fuerte). Por lo tanto, podemos hacer nuestra gráfica de residuales y ajustados,
agrupando los residuales, pero esta no sería una buena descripción de los datos.

<div class="caja">
**Gráfica de nivel contra dispersión.** Si en esta gráfica observamos patrones
fuertes (como dispersión monótona), concluimos que los residuales no tienen
dispersión homogénea. Usamos los valores ajustados porque queremos ver si la 
*distribución de los residuales depende o no de los valores ajustados* (y no de
los observados). Si no encontramos dependencia fuerte, es posible hacer
*pooling* para describir de manera más simple los datos: el observado es un 
valor ajustado más un residual con distribución conocida (que no depende del
ajustado).
</div>

A pesar de que encontramos dispersión no homogénea, examinamos la distribución 
de todos los residuales juntos, ¿se podrá describir con una distribución 
normal?

```{r, fig.width=3.6, fig.height=3.3}
ventas.sorteo.2 <- arrange(ventas.sorteo, residual)
ventas.sorteo.2$valor.f <- (1:nrow(ventas.sorteo.2) - 0.5)/nrow(ventas.sorteo.2)
ventas.sorteo.2$cnorm <- qnorm(ventas.sorteo.2$valor.f)

ggplot(ventas.sorteo.2, aes(x = cnorm, y = residual)) + 
  geom_point() +
  geom_smooth(method = "lm")
```

Muestra colas más largas que una distribución normal (es leptocúrtica), por
tanto la distribución normal no es apropiada para describir estos residuales.

Podemos usar los cuantiles de los residuales para describir la variación a 
partir de los valores ajustados. Esto lo hacemos con la gráfica de dispersión
de los ajustados y residuales.

```{r, fig.width=6.6, fig.height=3.3}
ventas.sorteo.2$ajustado.c <- ventas.sorteo.2$ajustado - 
  mean(ventas.sorteo.2$ajustado)

ventas.sorteo.m <- ventas.sorteo.2 %>%
  select(no_sorteo, ajustado.c, residual) %>%
  gather(variable, valor, -no_sorteo)
head(ventas.sorteo.m)
tail(ventas.sorteo.m)

ventas.ajuste.resid <- ventas.sorteo.m %>%
  group_by(variable) %>%
  arrange(valor) %>%
  mutate(
    n = n(),
    valor.f = (1:n[1] - 0.5) / n[1]
    )

ggplot(ventas.ajuste.resid, aes(x = valor.f, y = valor)) + 
  geom_point() + 
  facet_wrap(~variable)
```

Podemos dar deciles de la distribución de los residuales (no podemos usar la 
distribución normal), para tener una cuantificación de la variación alrededor
de los ajustados.

```{r}
round(quantile(ventas.sorteo$residual, probs = seq(0, 1, 0.1)), 2)
```

donde vemos que, a partir del ajustado, el valor observado no varía más de 
$\pm 0.09$ para el 80% de los datos.

